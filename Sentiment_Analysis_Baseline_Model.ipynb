{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Analyse de Sentiment Basée sur des Tweets - Modèle Baseline](#toc1_)    \n",
    "  - 1.1. [Préparation des Données](#toc1_1_)    \n",
    "  - 1.2. [Vectorisation des Tweets](#toc1_2_)    \n",
    "  - 1.3. [Entraînement et Évaluation du Modèle](#toc1_3_)    \n",
    "  - 1.4. [Comparaison des Résultats](#toc1_4_)    \n",
    "    - 1.4.1. [Comparaison des Performances entre TF-IDF et Count Vectorizer](#toc1_4_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Analyse de Sentiment Basée sur des Tweets - Modèle Baseline](#toc0_)\n",
    "\n",
    "Ce notebook présente un modèle de base pour l'analyse de sentiment de tweets en utilisant la **régression logistique**. Nous explorerons deux méthodes de vectorisation du texte : **TF-IDF** et **Count Vectorizer**, afin de convertir le texte en caractéristiques numériques exploitables par le modèle. \n",
    "\n",
    "Les principaux objectifs sont :\n",
    "- Préparer les données textuelles en utilisant des méthodes de vectorisation.\n",
    "- Entraîner un modèle de régression logistique sur les tweets vectorisés.\n",
    "- Suivre et évaluer les performances du modèle en utilisant **MLFlow** pour chaque méthode de vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliothèque pour la manipulation des données\n",
    "import pandas as pd\n",
    "\n",
    "# Bibliothèques pour la vectorisation et la transformation des textes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Modèle de régression logistique pour la classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Bibliothèques pour la division des données et les métriques d'évaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "# Bibliothèque pour le suivi des expérimentations\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Préparation des Données](#toc0_)\n",
    "\n",
    "Nous chargeons le dataset prétraité, qui contient les tweets et leur sentiment associé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données prétraitées\n",
    "df = pd.read_csv('data/processed_tweets.csv')\n",
    "X_text = df['text']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dived many time ball managed save 50 rest go b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  awww thats bummer shoulda got david carr third...\n",
       "1          0  upset cant update facebook texting might cry r...\n",
       "2          0  dived many time ball managed save 50 rest go b...\n",
       "3          0                    whole body feel itchy like fire\n",
       "4          0                           behaving im mad cant see"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///e:/Openclassroom/AI_Engineer/Projet_07/work_folder/mlruns/233432721080566223', creation_time=1731601110797, experiment_id='233432721080566223', last_update_time=1731601110797, lifecycle_stage='active', name='Sentiment_Analysis_Baseline_Model', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialiser MLFlow et Démarrer une Expérimentation\n",
    "mlflow.set_experiment(\"Sentiment_Analysis_Baseline_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Vectorisation des Tweets](#toc0_)\n",
    "\n",
    "Nous utilisons **TF-IDF** et **Count Vectorizer** pour créer des représentations numériques des tweets. TF-IDF pèse l’importance des mots en fonction de leur fréquence, tandis que Count Vectorizer compte les occurrences de chaque mot. Nous limiterons les caractéristiques à un maximum de 5000 mots pour optimiser le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les Features avec TF-IDF et Count Vectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
    "vectorizer_count = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Vectoriser les textes en utilisant TF-IDF\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(X_text)\n",
    "# Vectoriser les textes en utilisant Count Vectorizer\n",
    "X_count = vectorizer_count.fit_transform(X_text)\n",
    "\n",
    "# Diviser les données en jeu d'entraînement et de test pour chaque méthode\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "X_train_count, X_test_count, _, _ = train_test_split(X_count, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. <a id='toc1_3_'></a>[Entraînement et Évaluation du Modèle](#toc0_)\n",
    "\n",
    "Dans cette section, nous entraînons un modèle de **régression logistique** pour chaque méthode de vectorisation. Nous utilisons MLFlow pour suivre les expérimentations et enregistrer les métriques clés, telles que la précision, le F1-score et le ROC-AUC, afin de comparer les performances des deux méthodes de vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf431d66f8864959aa23775862bea895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for TF-IDF:\n",
      "Accuracy: 0.774974111149465\n",
      "ROC-AUC: 0.8550506385425456\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77    158794\n",
      "           1       0.77      0.79      0.78    159876\n",
      "\n",
      "    accuracy                           0.77    318670\n",
      "   macro avg       0.78      0.77      0.77    318670\n",
      "weighted avg       0.78      0.77      0.77    318670\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd603d6d449b4a1b972606b80b2c904c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Count Vectorizer:\n",
      "Accuracy: 0.7747356199202937\n",
      "ROC-AUC: 0.8492340943718327\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.74      0.77    158794\n",
      "           1       0.76      0.81      0.78    159876\n",
      "\n",
      "    accuracy                           0.77    318670\n",
      "   macro avg       0.78      0.77      0.77    318670\n",
      "weighted avg       0.78      0.77      0.77    318670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entraîner le Modèle et Suivre l'Expérimentation avec MLFlow\n",
    "# Utiliser MLFlow pour suivre les expérimentations avec les deux vectorisations\n",
    "for vectorizer_name, X_train, X_test in [(\"TF-IDF\", X_train_tfidf, X_test_tfidf), (\"Count Vectorizer\", X_train_count, X_test_count)]:\n",
    "    with mlflow.start_run():\n",
    "        # Entraîner le modèle de Régression Logistique\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Prédictions et Probabilités pour le ROC-AUC\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probabilité de la classe positive\n",
    "\n",
    "        # Évaluation des performances\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)  # Calcul du ROC-AUC\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        # Enregistrer les paramètres et les métriques dans MLFlow\n",
    "        mlflow.log_param(\"vectorizer\", vectorizer_name)\n",
    "        mlflow.log_param(\"model_type\", \"Logistic Regression\")\n",
    "        mlflow.log_param(\"max_features\", 5000)\n",
    "\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)  # Enregistrement du ROC-AUC\n",
    "        mlflow.log_metric(\"precision\", report[\"weighted avg\"][\"precision\"])\n",
    "        mlflow.log_metric(\"recall\", report[\"weighted avg\"][\"recall\"])\n",
    "        mlflow.log_metric(\"f1-score\", report[\"weighted avg\"][\"f1-score\"])\n",
    "\n",
    "        # Définir un exemple d'entrée pour MLFlow\n",
    "        input_example = X_test[0].toarray()  # Convertir le premier tweet vectorisé en format array\n",
    "\n",
    "        # Enregistrer le modèle dans MLFlow avec un exemple d'entrée\n",
    "        mlflow.sklearn.log_model(model, \"model\", input_example=input_example)\n",
    "\n",
    "        print(f\"Results for {vectorizer_name}:\")\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"ROC-AUC:\", roc_auc)\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. <a id='toc1_4_'></a>[Comparaison des Résultats](#toc0_)\n",
    "\n",
    "Nous comparons les performances obtenues avec TF-IDF et Count Vectorizer en utilisant les métriques enregistrées dans MLFlow. Cela nous permet de déterminer quelle méthode de vectorisation est la plus adaptée pour notre analyse de sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Count Vectorizer</td>\n",
       "      <td>0.774736</td>\n",
       "      <td>0.849234</td>\n",
       "      <td>0.775704</td>\n",
       "      <td>0.774736</td>\n",
       "      <td>0.774505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.774974</td>\n",
       "      <td>0.855051</td>\n",
       "      <td>0.775328</td>\n",
       "      <td>0.774974</td>\n",
       "      <td>0.774882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Vectorizer  Accuracy   ROC-AUC  Precision    Recall  F1-Score\n",
       "0  Count Vectorizer  0.774736  0.849234   0.775704  0.774736  0.774505\n",
       "1            TF-IDF  0.774974  0.855051   0.775328  0.774974  0.774882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nom de l'expérience\n",
    "experiment_name = \"Sentiment_Analysis_Baseline_Model\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Récupérer les exécutions de l'expérience\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id], order_by=[\"start_time DESC\"])\n",
    "\n",
    "# Extraire les métriques pour chaque vectorisation\n",
    "metrics = []\n",
    "for _, run in runs.iterrows():\n",
    "    metrics.append({\n",
    "        \"Vectorizer\": run[\"params.vectorizer\"],\n",
    "        \"Accuracy\": run[\"metrics.accuracy\"],\n",
    "        \"ROC-AUC\": run[\"metrics.roc_auc\"],\n",
    "        \"Precision\": run[\"metrics.precision\"],\n",
    "        \"Recall\": run[\"metrics.recall\"],\n",
    "        \"F1-Score\": run[\"metrics.f1-score\"]\n",
    "    })\n",
    "\n",
    "# Créer un DataFrame pour afficher les résultats\n",
    "comparison_df = pd.DataFrame(metrics)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. <a id='toc1_4_1_'></a>[Comparaison des Performances entre TF-IDF et Count Vectorizer](#toc0_)\n",
    "\n",
    "En comparant les deux méthodes de vectorisation :\n",
    "\n",
    "- **Précision** : Les deux vectorisations montrent une précision similaire.\n",
    "- **ROC-AUC** : TF-IDF montre un léger avantage en ROC-AUC, ce qui indique une meilleure capacité à distinguer les sentiments.\n",
    "- **F1-Score** : Les F1-scores sont également très proches, ce qui signifie que les deux méthodes capturent bien les tweets positifs et négatifs.\n",
    "\n",
    "Ces résultats indiquent que les deux vectorisations sont performantes, mais TF-IDF semble légèrement mieux adapté pour notre modèle de base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_7_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
