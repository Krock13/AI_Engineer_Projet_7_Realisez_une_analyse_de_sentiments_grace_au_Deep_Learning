{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [Analyse de Sentiment Basée sur des Tweets - Modèle Baseline](#toc1_)    \n",
    "  - 1.1. [Préparation des Données](#toc1_1_)    \n",
    "  - 1.2. [Vectorisation des Tweets](#toc1_2_)    \n",
    "  - 1.3. [Entraînement et Évaluation du Modèle](#toc1_3_)    \n",
    "  - 1.4. [Comparaison des Résultats](#toc1_4_)    \n",
    "    - 1.4.1. [Comparaison des Performances entre TF-IDF et Count Vectorizer](#toc1_4_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[Analyse de Sentiment Basée sur des Tweets - Modèle Baseline](#toc0_)\n",
    "\n",
    "Ce notebook présente un modèle de base pour l'analyse de sentiment de tweets en utilisant la **régression logistique**. Nous explorerons deux méthodes de vectorisation du texte : **TF-IDF** et **Count Vectorizer**, afin de convertir le texte en caractéristiques numériques exploitables par le modèle. \n",
    "\n",
    "Les principaux objectifs sont :\n",
    "- Préparer les données textuelles en utilisant des méthodes de vectorisation.\n",
    "- Entraîner un modèle de régression logistique sur les tweets vectorisés.\n",
    "- Suivre et évaluer les performances du modèle en utilisant **MLFlow** pour chaque méthode de vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulation de données et calculs\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Bibliothèques pour la vectorisation et la transformation des textes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Modèle de régression logistique pour la classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Bibliothèques pour la division des données et les métriques d'évaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "\n",
    "# Bibliothèque pour le suivi des expérimentations\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. <a id='toc1_1_'></a>[Préparation des Données](#toc0_)\n",
    "\n",
    "Nous chargeons le dataset prétraité, qui contient les tweets et leur sentiment associé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données prétraitées\n",
    "df = pd.read_csv('data/processed_tweets.csv')\n",
    "X_text = df['text']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww thats bummer shoulda got david carr third...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset cant update facebook texting might cry r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dived many time ball managed save 50 rest go b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>behaving im mad cant see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  awww thats bummer shoulda got david carr third...\n",
       "1          0  upset cant update facebook texting might cry r...\n",
       "2          0  dived many time ball managed save 50 rest go b...\n",
       "3          0                    whole body feel itchy like fire\n",
       "4          0                           behaving im mad cant see"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///e:/Openclassroom/AI_Engineer/Projet_07/work_folder/mlruns/233432721080566223', creation_time=1731601110797, experiment_id='233432721080566223', last_update_time=1731601110797, lifecycle_stage='active', name='Sentiment_Analysis_Baseline_Model', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialiser MLFlow et Démarrer une Expérimentation\n",
    "mlflow.set_experiment(\"Sentiment_Analysis_Baseline_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. <a id='toc1_2_'></a>[Vectorisation des Tweets](#toc0_)\n",
    "\n",
    "Nous utilisons **TF-IDF** et **Count Vectorizer** pour créer des représentations numériques des tweets. TF-IDF pèse l’importance des mots en fonction de leur fréquence, tandis que Count Vectorizer compte les occurrences de chaque mot. Nous limiterons les caractéristiques à un maximum de 5000 mots pour optimiser le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les Features avec TF-IDF et Count Vectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
    "vectorizer_count = CountVectorizer(max_features=5000)\n",
    "\n",
    "# Vectoriser les textes en utilisant TF-IDF\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(X_text)\n",
    "# Vectoriser les textes en utilisant Count Vectorizer\n",
    "X_count = vectorizer_count.fit_transform(X_text)\n",
    "\n",
    "# Diviser les données en jeu d'entraînement et de test pour chaque méthode\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "X_train_count, X_test_count, _, _ = train_test_split(X_count, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. <a id='toc1_3_'></a>[Entraînement et Évaluation du Modèle](#toc0_)\n",
    "\n",
    "Dans cette section, nous entraînons un modèle de **régression logistique** pour chaque méthode de vectorisation. Nous utilisons MLFlow pour suivre les expérimentations et enregistrer les métriques clés, telles que la précision, le F1-score et le ROC-AUC, afin de comparer les performances des deux méthodes de vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf19a84171d483e83c7f4b7b48c0abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for TF-IDF:\n",
      "Accuracy: 0.7750, ROC-AUC: 0.8551, Training Time: 2.86 seconds\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.77    158794\n",
      "           1       0.77      0.79      0.78    159876\n",
      "\n",
      "    accuracy                           0.78    318670\n",
      "   macro avg       0.78      0.77      0.77    318670\n",
      "weighted avg       0.78      0.78      0.77    318670\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69882cec2f9c4dc684730767036b1fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Count Vectorizer:\n",
      "Accuracy: 0.7747, ROC-AUC: 0.8492, Training Time: 4.46 seconds\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.74      0.77    158794\n",
      "           1       0.76      0.81      0.78    159876\n",
      "\n",
      "    accuracy                           0.77    318670\n",
      "   macro avg       0.78      0.77      0.77    318670\n",
      "weighted avg       0.78      0.77      0.77    318670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nommer l'expérience dans MLFlow\n",
    "mlflow.set_experiment(\"Sentiment_Analysis_Baseline_Model\")\n",
    "\n",
    "# Entraîner les modèles et suivre les expérimentations\n",
    "for vectorizer_name, X_train, X_test in [(\"TF-IDF\", X_train_tfidf, X_test_tfidf), (\"Count Vectorizer\", X_train_count, X_test_count)]:\n",
    "    with mlflow.start_run():\n",
    "        # Mesurer le temps d’entraînement\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Entraîner le modèle\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Calculer le temps écoulé\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Prédictions et Probabilités\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Évaluation des performances\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "        # Loguer les paramètres et métriques dans MLFlow\n",
    "        mlflow.log_param(\"vectorizer\", vectorizer_name)\n",
    "        mlflow.log_param(\"model_type\", \"Logistic Regression\")\n",
    "        mlflow.log_param(\"max_features\", 5000)\n",
    "\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"precision\", report[\"weighted avg\"][\"precision\"])\n",
    "        mlflow.log_metric(\"recall\", report[\"weighted avg\"][\"recall\"])\n",
    "        mlflow.log_metric(\"f1-score\", report[\"weighted avg\"][\"f1-score\"])\n",
    "        mlflow.log_metric(\"training_time_seconds\", elapsed_time)\n",
    "\n",
    "        # Exemple d'entrée pour MLFlow\n",
    "        input_example = X_test[0].toarray()  # Convertir le premier tweet vectorisé en format array\n",
    "\n",
    "        # Signature du modèle\n",
    "        signature = infer_signature(X_test, model.predict(X_test))\n",
    "\n",
    "        # Enregistrer le modèle avec signature et exemple\n",
    "        mlflow.sklearn.log_model(model, \"model\", input_example=input_example, signature=signature)\n",
    "\n",
    "        # Afficher les résultats dans la console\n",
    "        print(f\"Results for {vectorizer_name}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, ROC-AUC: {roc_auc:.4f}, Training Time: {elapsed_time:.2f} seconds\")\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. <a id='toc1_4_'></a>[Comparaison des Résultats](#toc0_)\n",
    "\n",
    "Nous comparons les performances obtenues avec TF-IDF et Count Vectorizer en utilisant les métriques enregistrées dans MLFlow. Cela nous permet de déterminer quelle méthode de vectorisation est la plus adaptée pour notre analyse de sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Count Vectorizer</td>\n",
       "      <td>0.774726</td>\n",
       "      <td>0.849216</td>\n",
       "      <td>0.775695</td>\n",
       "      <td>0.774726</td>\n",
       "      <td>0.774495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.775037</td>\n",
       "      <td>0.855077</td>\n",
       "      <td>0.775405</td>\n",
       "      <td>0.775037</td>\n",
       "      <td>0.774941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Vectorizer  Accuracy   ROC-AUC  Precision    Recall  F1-Score\n",
       "0  Count Vectorizer  0.774726  0.849216   0.775695  0.774726  0.774495\n",
       "1            TF-IDF  0.775037  0.855077   0.775405  0.775037  0.774941"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparer les résultats des runs dans MLFlow\n",
    "experiment_name = \"Sentiment_Analysis_Baseline_Model\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Récupérer les exécutions de l'expérience\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id], order_by=[\"start_time DESC\"])\n",
    "\n",
    "# Extraire les métriques pour chaque vectorisation\n",
    "metrics = []\n",
    "for _, run in runs.iterrows():\n",
    "    metrics.append({\n",
    "        \"Vectorizer\": run[\"params.vectorizer\"],\n",
    "        \"Accuracy\": run[\"metrics.accuracy\"],\n",
    "        \"ROC-AUC\": run[\"metrics.roc_auc\"],\n",
    "        \"Precision\": run[\"metrics.precision\"],\n",
    "        \"Recall\": run[\"metrics.recall\"],\n",
    "        \"F1-Score\": run[\"metrics.f1-score\"]\n",
    "    })\n",
    "\n",
    "# Créer un DataFrame pour afficher les résultats\n",
    "comparison_df = pd.DataFrame(metrics)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. <a id='toc1_4_1_'></a>[Comparaison des Performances entre TF-IDF et Count Vectorizer](#toc0_)\n",
    "\n",
    "En comparant les deux méthodes de vectorisation :\n",
    "\n",
    "- **Précision** : Les deux vectorisations montrent une précision similaire.\n",
    "- **ROC-AUC** : TF-IDF montre un léger avantage en ROC-AUC, ce qui indique une meilleure capacité à distinguer les sentiments.\n",
    "- **F1-Score** : Les F1-scores sont également très proches, ce qui signifie que les deux méthodes capturent bien les tweets positifs et négatifs.\n",
    "\n",
    "Ces résultats indiquent que les deux vectorisations sont performantes, mais TF-IDF semble légèrement mieux adapté pour notre modèle de base."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_7_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
