{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Avancée des Sentiments avec des Modèles LSTM\n",
    "\n",
    "Ce notebook a pour objectif de développer et comparer des modèles avancés pour l'analyse des sentiments appliquée à des tweets.\n",
    "Il constitue une avancée par rapport à l'approche de base utilisant une régression logistique, en explorant des architectures de deep learning plus complexes.\n",
    "\n",
    "## Objectifs :\n",
    "1. Mettre en œuvre et entraîner des modèles **Long Short-Term Memory (LSTM)** pour la classification des sentiments.\n",
    "2. Évaluer les performances de deux méthodes d’embedding :\n",
    "   - **Word2Vec** : Embeddings pré-entraînés capturant le contexte et la signification des mots.\n",
    "   - **FastText** : Embeddings pré-entraînés intégrant des informations sur les sous-mots, utiles pour les mots rares ou mal orthographiés.\n",
    "3. Suivre et analyser les performances des modèles grâce à **MLFlow**, en se concentrant sur des métriques telles que :\n",
    "   - Précision (Validation Accuracy)\n",
    "   - Perte (Validation Loss)\n",
    "   - ROC-AUC\n",
    "\n",
    "## Jeu de données et déroulement :\n",
    "- **Jeu de données** : Dataset de tweets prétraités contenant des labels binaires (positif ou négatif).\n",
    "- **Étapes** :\n",
    "  1. Tokenisation et padding des tweets.\n",
    "  2. Intégration des embeddings Word2Vec et FastText pré-entraînés.\n",
    "  3. Construction et entraînement des modèles LSTM.\n",
    "  4. Évaluation et comparaison des modèles à l’aide des données de validation et de test.\n",
    "\n",
    "Ce notebook reflète une approche professionnelle et avancée pour les tâches de classification de texte en exploitant des techniques modernes de NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulation de données et calculs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Outils pour la gestion des ensembles de données et l'évaluation des modèles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Bibliothèques pour la construction et l'entraînement des modèles\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Chargement des embeddings pré-entraînés (Word2Vec, FastText)\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Suivi et enregistrement des expérimentations avec MLFlow\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le dataset prétraité\n",
    "df = pd.read_csv('../data/processed_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données\n",
    "X = df['text']\n",
    "y = df['sentiment']\n",
    "\n",
    "# Tokenizer les tweets\n",
    "tokenizer = Tokenizer(num_words=20000)  # Limiter à 20 000 mots\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_pad = pad_sequences(X_seq, maxlen=100)  # Fixer une longueur maximale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division des données en ensembles d'entraînement, validation et test\n",
    "\n",
    "Afin de garantir une évaluation rigoureuse et éviter tout biais, les données sont divisées en trois ensembles distincts :\n",
    "\n",
    "1. **Jeu de Test** :\n",
    "   - 20% des données sont réservées pour le test final du modèle.\n",
    "   - Ce jeu de données ne sera utilisé qu’à la fin pour évaluer objectivement les performances.\n",
    "\n",
    "2. **Jeu de Validation** :\n",
    "   - Sur les 80% restants (le jeu d’entraînement complet), 20% sont extraits pour constituer le jeu de validation.\n",
    "   - Le jeu de validation est utilisé pendant l’entraînement pour ajuster les hyperparamètres et surveiller les performances sans sur-apprendre sur les données d’entraînement.\n",
    "\n",
    "3. **Jeu d’Entraînement Final** :\n",
    "   - Les 64% restants des données initiales sont utilisés pour entraîner le modèle.\n",
    "\n",
    "Cette double division permet de maintenir une distinction claire entre les données utilisées pour l’entraînement, la validation et le test, en respectant les bonnes pratiques pour une évaluation fiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division en jeu d'entraînement, validation et test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger Word2Vec pré-entraîné\n",
    "word2vec = KeyedVectors.load_word2vec_format('../models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Créer une matrice d'embedding pour Word2Vec\n",
    "embedding_matrix_w2v = np.zeros((20000, word2vec.vector_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < 20000 and word in word2vec:\n",
    "        embedding_matrix_w2v[i] = word2vec[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger FastText pré-entraîné\n",
    "fasttext = KeyedVectors.load_word2vec_format('../models/crawl-300d-2M-subword.vec')\n",
    "\n",
    "# Créer une matrice d'embedding pour FastText\n",
    "embedding_matrix_ft = np.zeros((20000, fasttext.vector_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < 20000 and word in fasttext:\n",
    "        embedding_matrix_ft[i] = fasttext[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du modèle LSTM\n",
    "\n",
    "La fonction `create_lstm_model` construit un modèle basé sur une architecture **LSTM** pour l'analyse des sentiments. \n",
    "\n",
    "- **Embedding** : \n",
    "  - Intègre une matrice d'embedding pré-entraînée (Word2Vec ou FastText) pour représenter les mots dans un espace vectoriel.\n",
    "  - Les poids de la matrice sont fixés (`trainable=False`) pour conserver les embeddings d'origine.\n",
    "\n",
    "- **LSTM Layer** :\n",
    "  - Une couche LSTM (Long Short-Term Memory) est utilisée pour capturer les dépendances contextuelles dans les tweets.\n",
    "\n",
    "- **Dense Layer** :\n",
    "  - Une couche de sortie entièrement connectée avec une activation `sigmoid` pour produire une probabilité binaire (positif ou négatif).\n",
    "\n",
    "- **Compilation** :\n",
    "  - Le modèle est compilé avec l'optimiseur `adam` et une fonction de perte adaptée à la classification binaire (`binary_crossentropy`).\n",
    "\n",
    "Cette fonction permet de construire dynamiquement des modèles adaptés à différentes matrices d’embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(embedding_matrix):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=20000, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False),\n",
    "        LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/19 15:34:09 INFO mlflow.tracking.fluent: Experiment with name 'Sentiment_Analysis_Advanced_Model' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1018s\u001b[0m 32ms/step - accuracy: 0.7480 - loss: 0.5072 - val_accuracy: 0.7852 - val_loss: 0.4528\n",
      "Epoch 2/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m994s\u001b[0m 31ms/step - accuracy: 0.7850 - loss: 0.4536 - val_accuracy: 0.7917 - val_loss: 0.4437\n",
      "Epoch 3/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1006s\u001b[0m 32ms/step - accuracy: 0.7927 - loss: 0.4413 - val_accuracy: 0.7937 - val_loss: 0.4396\n",
      "Epoch 4/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m998s\u001b[0m 31ms/step - accuracy: 0.7956 - loss: 0.4361 - val_accuracy: 0.7948 - val_loss: 0.4385\n",
      "Epoch 5/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1010s\u001b[0m 32ms/step - accuracy: 0.7994 - loss: 0.4297 - val_accuracy: 0.7958 - val_loss: 0.4365\n",
      "\u001b[1m7967/7967\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 11ms/step\n",
      "\u001b[1m7967/7967\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 10ms/step\n",
      "Modèle Word2Vec enregistré avec succès dans MLFlow.\n",
      "Epoch 1/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m892s\u001b[0m 28ms/step - accuracy: 0.7518 - loss: 0.5053 - val_accuracy: 0.7868 - val_loss: 0.4507\n",
      "Epoch 2/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m870s\u001b[0m 27ms/step - accuracy: 0.7850 - loss: 0.4548 - val_accuracy: 0.7947 - val_loss: 0.4388\n",
      "Epoch 3/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m883s\u001b[0m 28ms/step - accuracy: 0.7929 - loss: 0.4416 - val_accuracy: 0.7976 - val_loss: 0.4334\n",
      "Epoch 4/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m910s\u001b[0m 29ms/step - accuracy: 0.7876 - loss: 0.4501 - val_accuracy: 0.7367 - val_loss: 0.5260\n",
      "Epoch 5/5\n",
      "\u001b[1m31867/31867\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m950s\u001b[0m 30ms/step - accuracy: 0.7629 - loss: 0.4891 - val_accuracy: 0.7958 - val_loss: 0.4369\n",
      "\u001b[1m7967/7967\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 10ms/step\n",
      "\u001b[1m7967/7967\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 10ms/step\n",
      "Modèle FastText enregistré avec succès dans MLFlow.\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Sentiment_Analysis_Advanced_Model\")\n",
    "\n",
    "for embedding_name, embedding_matrix in [(\"Word2Vec\", embedding_matrix_w2v), (\"FastText\", embedding_matrix_ft)]:\n",
    "    with mlflow.start_run():\n",
    "        # Loguer les hyperparamètres communs\n",
    "        mlflow.log_param(\"embedding\", embedding_name)\n",
    "        mlflow.log_param(\"batch_size\", 32)\n",
    "        mlflow.log_param(\"epochs\", 5)\n",
    "        mlflow.log_param(\"max_sequence_length\", 100)\n",
    "\n",
    "        # Créer le modèle\n",
    "        model = Sequential([\n",
    "            Embedding(input_dim=20000, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False),\n",
    "            LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Chronométrer l'entraînement\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Entraîner le modèle\n",
    "        history = model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val))\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Calculer les métriques finales\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        roc_auc = roc_auc_score(y_val, model.predict(X_val))\n",
    "\n",
    "        # Loguer les métriques\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"training_time_seconds\", elapsed_time)\n",
    "\n",
    "        # Inférer la signature\n",
    "        signature = infer_signature(X_val, model.predict(X_val))\n",
    "\n",
    "        try:\n",
    "            # Enregistrer le modèle directement dans MLFlow\n",
    "            mlflow.keras.log_model(\n",
    "                model=model,\n",
    "                artifact_path=\"model\",\n",
    "                signature=signature\n",
    "            )\n",
    "            print(f\"Modèle {embedding_name} enregistré avec succès dans MLFlow.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'enregistrement du modèle {embedding_name} : {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_7_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
