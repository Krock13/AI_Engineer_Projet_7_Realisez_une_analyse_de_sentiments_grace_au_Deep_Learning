{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Openclassroom\\AI_Engineer\\Projet_07\\work_folder\\projet_7_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manipulation de données et calculs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Outils pour la gestion des ensembles de données et l'évaluation des modèles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Bibliothèques pour la construction et l'entraînement des modèles\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Suivi et enregistrement des expérimentations avec MLFlow\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "0    1000\n",
      "1    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "data_path = 'data/training.1600000.processed.noemoticon.csv'\n",
    "df = pd.read_csv(data_path, encoding='ISO-8859-1', header=None)\n",
    "\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Mapper les sentiments à des valeurs binaires\n",
    "df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "\n",
    "# Garder uniquement les colonnes utiles\n",
    "df = df[['sentiment', 'text']]\n",
    "\n",
    "# Échantillonnage équilibré\n",
    "sample_size = 200_000  # Nombre de tweets par classe\n",
    "df_positive = df[df['sentiment'] == 1].sample(n=sample_size, random_state=42)\n",
    "df_negative = df[df['sentiment'] == 0].sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Combiner les deux échantillons\n",
    "df_sampled = pd.concat([df_positive, df_negative]).sample(frac=1, random_state=42)  # Mélanger les tweets\n",
    "\n",
    "# Vérifier la répartition\n",
    "print(df_sampled['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer un nettoyage simple\n",
    "def preprocess_tweet_for_sentiment(text):\n",
    "    # Supprimer les mentions @pseudo\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Supprimer les espaces superflus\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_sampled['text'] = df_sampled['text'].apply(preprocess_tweet_for_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1600, 59), X_test: (400, 59)\n",
      "y_train: (1600,), y_test: (400,)\n"
     ]
    }
   ],
   "source": [
    "# Charger le tokenizer Bert\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizer les tweets échantillonnés\n",
    "X_tokenized = tokenizer(\n",
    "    list(df_sampled['text']),  # Utiliser les données échantillonnées\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=100,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# Préparer les labels\n",
    "y = tf.convert_to_tensor(df_sampled['sentiment'].values)\n",
    "\n",
    "# Diviser en jeu d'entraînement et de test\n",
    "X_train = {\n",
    "    key: value[:int(0.8 * len(value))] for key, value in X_tokenized.items()\n",
    "}\n",
    "X_test = {\n",
    "    key: value[int(0.8 * len(value)):] for key, value in X_tokenized.items()\n",
    "}\n",
    "y_train = y[:int(0.8 * len(y))]\n",
    "y_test = y[int(0.8 * len(y)):]\n",
    "\n",
    "# Vérifier les dimensions\n",
    "print(f\"X_train: {X_train['input_ids'].shape}, X_test: {X_test['input_ids'].shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Openclassroom\\AI_Engineer\\Projet_07\\work_folder\\projet_7_env\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle Bert pour la classification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Définir l'optimiseur et la fonction de perte\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des étapes totales et warm-up steps\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "steps_per_epoch = len(y_train) // batch_size\n",
    "total_training_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(0.1 * total_training_steps)\n",
    "\n",
    "# Créer un optimiseur compatible avec Transformers\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,  # Taux d'apprentissage initial\n",
    "    num_train_steps=total_training_steps,  # Étapes totales d'entraînement\n",
    "    num_warmup_steps=warmup_steps,  # Étapes de warm-up\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From e:\\Openclassroom\\AI_Engineer\\Projet_07\\work_folder\\projet_7_env\\lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Openclassroom\\AI_Engineer\\Projet_07\\work_folder\\projet_7_env\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "40/40 [==============================] - 75s 2s/step - loss: 0.6788 - accuracy: 0.5672 - val_loss: 0.5961 - val_accuracy: 0.7406\n",
      "Epoch 2/3\n",
      "40/40 [==============================] - 63s 2s/step - loss: 0.5257 - accuracy: 0.7586 - val_loss: 0.4822 - val_accuracy: 0.7906\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 64s 2s/step - loss: 0.4004 - accuracy: 0.8383 - val_loss: 0.4786 - val_accuracy: 0.7781\n",
      "13/13 [==============================] - 7s 412ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Openclassroom\\AI_Engineer\\Projet_07\\work_folder\\projet_7_env\\lib\\site-packages\\transformers\\generation\\tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT - Validation Accuracy: 0.7781, Loss: 0.4786, ROC-AUC: 0.8646, Training Time: 202.03s\n"
     ]
    }
   ],
   "source": [
    "# Définir l'expérience MLFlow\n",
    "mlflow.set_experiment(\"Sentiment_Analysis_BERT_Model\")\n",
    "\n",
    "# Entraîner BERT\n",
    "with mlflow.start_run():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Compiler le modèle avec l'optimiseur compatible\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Évaluer les performances\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    y_pred = model.predict(X_test).logits\n",
    "    roc_auc = roc_auc_score(y_test.numpy(), tf.nn.softmax(y_pred)[:, 1].numpy())\n",
    "\n",
    "    # Logger les paramètres et métriques dans MLFlow\n",
    "    mlflow.log_param(\"model\", \"BERT\")\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_metric(\"val_accuracy\", val_accuracy)\n",
    "    mlflow.log_metric(\"val_loss\", val_loss)\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    mlflow.log_metric(\"training_time_seconds\", elapsed_time)\n",
    "\n",
    "    # Préparer les données de test pour la signature\n",
    "    X_test_combined = np.hstack([\n",
    "        X_test[\"input_ids\"].numpy(),\n",
    "        X_test[\"attention_mask\"].numpy(),\n",
    "        X_test[\"token_type_ids\"].numpy(),\n",
    "    ])\n",
    "\n",
    "    # Définir une signature pour le modèle\n",
    "    signature = infer_signature(X_test_combined, y_pred)\n",
    "\n",
    "    # Enregistrer le modèle avec signature dans MLFlow\n",
    "    mlflow.keras.log_model(\n",
    "        model=model,\n",
    "        artifact_path=\"model\",\n",
    "        signature=signature,\n",
    "        pip_requirements=\"requirements.txt\"\n",
    "    )\n",
    "\n",
    "    print(f\"BERT - Validation Accuracy: {val_accuracy:.4f}, Loss: {val_loss:.4f}, ROC-AUC: {roc_auc:.4f}, Training Time: {elapsed_time:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_7_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
